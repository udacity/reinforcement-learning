{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discretization\n",
    "\n",
    "In this notebook, you will deal with continuous state/action spaces by discretizing them. This will enable you to apply reinforcement learning algorithms that are only designed to work with discrete spaces.\n",
    "\n",
    "> **Tip**: You can run each code block below by pressing **`Shift+Enter`**. Look for any **`TODO`** comments and implement the indicated code. Also try to answer any questions (**Q:**) by editing the corresponding answer (**A:**) block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import common libraries\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from IPython import display\n",
    "\n",
    "# Set plotting options\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "np.set_printoptions(precision=3, linewidth=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use [OpenAI Gym](https://gym.openai.com/) environments to test and develop our algorithms. These simulate a variety of classic as well as contemporary reinforcement learning tasks.\n",
    "\n",
    "> **Note**: You can run this notebook locally (use `render_mode = 'desktop'`), or in an online workspace / cloud instance (use `render_mode = 'jupyter'`, to visualize the simulation within the notebook). But visualization can be quite slow (esp. in the notebook), so you may want to set it to `None` to suppress visualization when you're ready to run a long batch. It is always recommended to download and run the notebook on your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Global render settings\n",
    "render_mode = 'desktop'  # set to 'desktop' (when running on local machine) or 'jupyter' (in notebook, slow)\n",
    "\n",
    "# Start a virtual display if you wish to render within the jupyter notebook (needed for certain envs)\n",
    "if render_mode == 'jupyter':\n",
    "    !pip install pyvirtualdisplay  # install pyvirtualdisplay, one-time only\n",
    "    from pyvirtualdisplay import Display\n",
    "    virtual_display = Display(visible=0)\n",
    "    virtual_display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous State Space\n",
    "\n",
    "Let's begin with an environment that has a continuous state space, but a discrete action space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create an environment\n",
    "env = gym.make('MountainCar-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can explore the state and action spaces, as well as sample them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Explore state (observation) space\n",
    "print(\"State space:\", env.observation_space)\n",
    "print(\"- low:\", env.observation_space.low)\n",
    "print(\"- high:\", env.observation_space.high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate some samples from state space (if the space is finite)\n",
    "if np.all(np.isfinite(env.observation_space.low)) and np.all(np.isfinite(env.observation_space.high)):\n",
    "    print(\"State space samples:\")\n",
    "    print(np.array([env.observation_space.sample() for i in range(10)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Explore action space\n",
    "print(\"Action space:\", env.action_space)\n",
    "\n",
    "# Generate some samples from action space\n",
    "print(\"Action space samples:\")\n",
    "print(np.array([env.action_space.sample() for i in range(10)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uniform Grid\n",
    "\n",
    "The first approach we'll try is to discretize the space using a uniformly-spaced grid. Implement the following function to create such a grid, given the lower bounds (`low`), upper bounds (`high`), and number of desired `bins` along each dimension. It should return the split points for each dimension, which will be 1 less than the number of bins.\n",
    "\n",
    "E.g. if `low = [-1.0, -5.0]`, `high = [1.0, 5.0]`, and `bins = [10, 10]`, then return a NumPy array containing the following split points (2 dimensions, 9 split points per dimension):\n",
    "\n",
    "```\n",
    "[[-0.8, -0.6, -0.4, -0.2, 0.0, 0.2, 0.4, 0.6, 0.8],\n",
    " [-4.0, -3.0, -2.0, -1.0, 0.0, 1.0, 2.0, 3.0, 4.0]]\n",
    "```\n",
    "\n",
    "Note that the ends of `low` and `high` are **not** included in these split points. It is assumed that any value below the lowest split point maps to index `0` and any value above the highest split point maps to index `n-1`, where `n` is the number of bins along that dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_uniform_grid(low, high, bins=(10, 10)):\n",
    "    \"\"\"Define a uniformly-spaced grid that can be used to discretize a space.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    low : array_like\n",
    "        Lower bounds for each dimension of the continuous space.\n",
    "    high : array_like\n",
    "        Upper bounds for each dimension of the continuous space.\n",
    "    bins : tuple\n",
    "        Number of bins along each corresponding dimension.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    grid : list of array_like\n",
    "        A list of arrays containing split points for each dimension.\n",
    "    \"\"\"\n",
    "    # TODO: Implement this\n",
    "    pass\n",
    "\n",
    "\n",
    "low = [-1.0, -5.0]\n",
    "high = [1.0, 5.0]\n",
    "create_uniform_grid(low, high)  # [test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now write a function that can convert samples from a continuous space into its equivalent discretized representation, given a grid like the one you created above. You can use the [`numpy.digitize()`](https://docs.scipy.org/doc/numpy-1.9.3/reference/generated/numpy.digitize.html) function for this purpose.\n",
    "\n",
    "Assume the grid is a NumPy array containing the following split points:\n",
    "```\n",
    "[[-0.8, -0.6, -0.4, -0.2, 0.0, 0.2, 0.4, 0.6, 0.8],\n",
    " [-4.0, -3.0, -2.0, -1.0, 0.0, 1.0, 2.0, 3.0, 4.0]]\n",
    "```\n",
    "\n",
    "Here are some potential samples and their corresponding discretized representations:\n",
    "```\n",
    "[-1.0 , -5.0] => [0, 0]\n",
    "[-0.81, -4.1] => [0, 0]\n",
    "[-0.8 , -4.0] => [1, 1]\n",
    "[-0.5 ,  0.0] => [2, 5]\n",
    "[ 0.2 , -1.9] => [6, 3]\n",
    "[ 0.8 ,  4.0] => [9, 9]\n",
    "[ 0.81,  4.1] => [9, 9]\n",
    "[ 1.0 ,  5.0] => [9, 9]\n",
    "```\n",
    "\n",
    "Note: There may be one-off differences in binning due to floating-point inaccuracies when samples are close to grid boundaries, but that is alright."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def discretize(sample, grid):\n",
    "    \"\"\"Discretize a sample as per given grid.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sample : array_like\n",
    "        A single sample from the (original) continuous space.\n",
    "    grid : list of array_like\n",
    "        A list of arrays containing split points for each dimension.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    discretized_sample : array_like\n",
    "        A sequence of integers with the same number of dimensions as sample.\n",
    "    \"\"\"\n",
    "    # TODO: Implement this\n",
    "    pass\n",
    "\n",
    "\n",
    "# Test with a simple grid and some samples\n",
    "grid = create_uniform_grid([-1.0, -5.0], [1.0, 5.0])\n",
    "samples = np.array(\n",
    "    [[-1.0 , -5.0],\n",
    "     [-0.81, -4.1],\n",
    "     [-0.8 , -4.0],\n",
    "     [-0.5 ,  0.0],\n",
    "     [ 0.2 , -1.9],\n",
    "     [ 0.8 ,  4.0],\n",
    "     [ 0.81,  4.1],\n",
    "     [ 1.0 ,  5.0]])\n",
    "discretized_samples = np.array([discretize(sample, grid) for sample in samples])\n",
    "print(\"\\nSamples:\", repr(samples), sep=\"\\n\")\n",
    "print(\"\\nDiscretized samples:\", repr(discretized_samples), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "It might be helpful to visualize the original and discretized samples to get a sense of how much error you are introducing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.collections as mc\n",
    "\n",
    "def visualize_samples(samples, discretized_samples, grid, low=None, high=None):\n",
    "    \"\"\"Visualize original and discretized samples on a given 2-dimensional grid.\"\"\"\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    \n",
    "    # Show grid\n",
    "    ax.xaxis.set_major_locator(plt.FixedLocator(grid[0]))\n",
    "    ax.yaxis.set_major_locator(plt.FixedLocator(grid[1]))\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # If bounds (low, high) are specified, use them to set axis limits\n",
    "    if low is not None and high is not None:\n",
    "        ax.set_xlim(low[0], high[0])\n",
    "        ax.set_ylim(low[1], high[1])\n",
    "    else:\n",
    "        # Otherwise use first, last grid locations as low, high (for further mapping discretized samples)\n",
    "        low = [splits[0] for splits in grid]\n",
    "        high = [splits[-1] for splits in grid]\n",
    "\n",
    "    # Map each discretized sample (which is really an index) to the center of corresponding grid cell\n",
    "    grid_extended = np.hstack((np.array([low]).T, grid, np.array([high]).T))  # add low and high ends\n",
    "    grid_centers = (grid_extended[:, 1:] + grid_extended[:, :-1]) / 2  # compute center of each grid cell\n",
    "    locs = np.stack(grid_centers[i, discretized_samples[:, i]] for i in range(len(grid))).T  # map discretized samples\n",
    "\n",
    "    ax.plot(samples[:, 0], samples[:, 1], 'o')  # plot original samples\n",
    "    ax.plot(locs[:, 0], locs[:, 1], 's')  # plot discretized samples in mapped locations\n",
    "    ax.add_collection(mc.LineCollection(list(zip(samples, locs)), colors='orange'))  # add a line connecting each original-discretized sample\n",
    "    ax.legend(['original', 'discretized'])\n",
    "\n",
    "\n",
    "visualize_samples(samples, discretized_samples, grid, low, high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a way to discretize a state space, let's apply it to our reinforcement learning environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a grid to discretize the state space\n",
    "state_grid = create_uniform_grid(env.observation_space.low, env.observation_space.high, bins=(10, 10))\n",
    "state_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Obtain some samples from the space, discretize them, and then visualize them\n",
    "state_samples = np.array([env.observation_space.sample() for i in range(10)])\n",
    "discretized_state_samples = np.array([discretize(sample, state_grid) for sample in state_samples])\n",
    "visualize_samples(state_samples, discretized_state_samples, state_grid,\n",
    "                  env.observation_space.low, env.observation_space.high)\n",
    "plt.xlabel('position'); plt.ylabel('velocity');  # axis labels for MountainCar-v0 state space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you might've noticed is that if you have enough bins, the discretization doesn't introduce too much error into your representation. So we may be able to now apply a reinforcement learning algorithm (like Q-Learning) that operates on discrete spaces. Give it a shot to see how well it works!\n",
    "\n",
    "## Q-Learning\n",
    "\n",
    "Provided below is a simple Q-Learning agent. Implement the `preprocess_state()` method to convert each continuous state sample to to its corresponding discretized representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    \"\"\"Q-Learning agent that can act on a continuous state space by discretizing it.\"\"\"\n",
    "\n",
    "    def __init__(self, env, state_grid,\n",
    "                 alpha=0.2, gamma=0.9,\n",
    "                 epsilon=0.5, epsilon_decay_rate=0.99):\n",
    "        \"\"\"Initialize variables, create grid for discretization.\"\"\"\n",
    "        # Environment info\n",
    "        self.env = env\n",
    "        self.state_grid = state_grid\n",
    "        self.state_size = tuple(len(splits) + 1 for splits in self.state_grid)  # n-dimensional state space\n",
    "        self.action_size = self.env.action_space.n  # 1-dimensional discrete action space\n",
    "        print(\"Environment:\", self.env)\n",
    "        print(\"State space size:\", self.state_size)\n",
    "        print(\"Action space size:\", self.action_size)\n",
    "        \n",
    "        # Learning parameters\n",
    "        self.alpha = alpha  # learning rate\n",
    "        self.gamma = gamma  # discount factor\n",
    "        self.epsilon = self.initial_epsilon = epsilon  # initial exploration rate\n",
    "        self.epsilon_decay_rate = epsilon_decay_rate # how quickly should we decrease epsilon\n",
    "\n",
    "        # Create Q-table\n",
    "        self.q_table = np.zeros(shape=(self.state_size + (self.action_size,)))\n",
    "        print(\"Q table size:\", self.q_table.shape)\n",
    "\n",
    "    def preprocess_state(self, state):\n",
    "        \"\"\"Map a continuous state to its discretized representation.\"\"\"\n",
    "        # TODO: Implement this\n",
    "        pass\n",
    "\n",
    "    def reset_episode(self, state):\n",
    "        \"\"\"Reset variables for a new episode.\"\"\"\n",
    "        # Gradually decrease exploration rate\n",
    "        self.epsilon *= self.epsilon_decay_rate\n",
    "\n",
    "        # Decide initial action\n",
    "        self.last_state = self.preprocess_state(state)\n",
    "        self.last_action = np.argmax(self.q_table[self.last_state])\n",
    "        return self.last_action\n",
    "    \n",
    "    def reset_exploration(self, epsilon=None):\n",
    "        \"\"\"Reset exploration rate used when training.\"\"\"\n",
    "        self.epsilon = epsilon if epsilon is not None else self.initial_epsilon\n",
    "\n",
    "    def act(self, state, reward=None, done=None, mode='train'):\n",
    "        \"\"\"Pick next action and update internal Q table (when mode != 'test').\"\"\"\n",
    "        state = self.preprocess_state(state)\n",
    "        if mode == 'test':\n",
    "            # Test mode: Simply produce an action\n",
    "            action = np.argmax(self.q_table[state])\n",
    "        else:\n",
    "            # Train mode (default): Update Q table, pick next action\n",
    "            # Note: We update the Q table entry for the *last* (state, action) pair with current state, reward\n",
    "            self.q_table[self.last_state + (self.last_action,)] += self.alpha * \\\n",
    "                (reward + self.gamma * max(self.q_table[state]) - self.q_table[self.last_state + (self.last_action,)])\n",
    "\n",
    "            # Exploration vs. exploitation\n",
    "            do_exploration = np.random.uniform(0, 1) < self.epsilon\n",
    "            if do_exploration:\n",
    "                # Pick a random action\n",
    "                action = np.random.randint(0, self.action_size)\n",
    "            else:\n",
    "                # Pick the best action from Q table\n",
    "                action = np.argmax(self.q_table[state])\n",
    "\n",
    "        # Roll over current state, action for next step\n",
    "        self.last_state = state\n",
    "        self.last_action = action\n",
    "        return action\n",
    "\n",
    "\n",
    "q_agent = QLearningAgent(env, state_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also define a convenience function to run an agent on a given environment. We can then reuse it later.\n",
    "\n",
    "> **Note**: When calling this function, you can pass in `mode='test'` to tell the agent not to learn. You can also pass in `render_mode=render_mode` to use the global mode defined earlier, or override it, e.g. to suppress visualization altogether, use `render_mode=None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run(agent, env, num_episodes=1000, mode='train',\n",
    "        render_mode='desktop', render_every=100, jupyter_frame_interval=15):\n",
    "    \"\"\"Run agent in given reinforcement learning environment and return episode rewards.\"\"\"\n",
    "    episode_rewards = []\n",
    "\n",
    "    try:\n",
    "        for e in tqdm(range(num_episodes), disable=(render_mode=='jupyter')):\n",
    "            # Initialize episode\n",
    "            state = env.reset()\n",
    "            action = agent.reset_episode(state)\n",
    "            t = 0\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "\n",
    "            if render_mode == 'jupyter':\n",
    "                img = plt.imshow(env.render(mode='rgb_array'))  # [jupyter] one-time only\n",
    "                plt.axis('off')\n",
    "\n",
    "            # Roll out steps till done\n",
    "            while not done:\n",
    "                state, reward, done, info = env.step(action)\n",
    "                total_reward += reward\n",
    "                action = agent.act(state, reward, done, mode)\n",
    "                if render_mode and e % render_every == 0:  # only display certain episodes\n",
    "                    if render_mode == 'jupyter':\n",
    "                        if t % jupyter_frame_interval == 0:\n",
    "                            img.set_data(env.render(mode='rgb_array'))  # [jupyter] just update the data\n",
    "                            display.display(plt.gcf())\n",
    "                            display.clear_output(wait=True)\n",
    "                    else:\n",
    "                        env.render(mode='human', close=done)\n",
    "                t += 1\n",
    "\n",
    "            # Collect episode reward\n",
    "            episode_rewards.append(total_reward)\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    finally:\n",
    "        if render_mode:\n",
    "            env.render(close=True)\n",
    "\n",
    "    return episode_rewards\n",
    "\n",
    "\n",
    "episode_rewards = run(q_agent, env, num_episodes=1000, render_every=100, render_mode=render_mode)\n",
    "print(\"Completed {} episodes with avg. reward = {}\".format(len(episode_rewards), np.mean(episode_rewards)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best way to analyze if your agent was learning the task is to plot episode rewards. It should generally increase as the agent goes through more episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot rewards obtained per episode\n",
    "plt.plot(episode_rewards); plt.title(\"Episode rewards\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the episode rewards are noisy, it might be difficult to tell whether your agent is actually learning. To find the underlying trend, you may want to plot a rolling mean of the episode rewards. Let's write a convenience function to plot both raw rewards as well as a rolling mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_rewards(rewards, rolling_window=None):\n",
    "    \"\"\"Plot rewards and optional rolling mean using specified window.\"\"\"\n",
    "    plt.plot(rewards); plt.title(\"Rewards\");\n",
    "    if rolling_window is not None:\n",
    "        plt.plot(pd.Series(rewards).rolling(rolling_window).mean());\n",
    "\n",
    "\n",
    "window_size = 50  # choose a suitable window size, e.g. int(len(episode_rewards) / 25)\n",
    "plot_rewards(episode_rewards, rolling_window=window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you haven't yet reached a desired level of performance, try training your agent a few more times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run in training mode for some more episodes and analyze rewards\n",
    "# Note: In this case, we want to keep the entire history of episode rewards\n",
    "episode_rewards += run(q_agent, env, num_episodes=1000, render_mode=None)\n",
    "print(\"Completed {} total episodes with avg. reward = {}\".format(len(episode_rewards), np.mean(episode_rewards)))\n",
    "plot_rewards(episode_rewards, rolling_window=window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should observe the mean episode rewards go up over time. Finally, you can freeze learning and run the agent in test mode to see how well it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run in test mode and analyze rewards obtained\n",
    "test_rewards = run(q_agent, env, num_episodes=100, mode='test', render_mode=render_mode, render_every=10)\n",
    "print(\"[TEST] Completed {} episodes with avg. reward = {}\".format(len(test_rewards), np.mean(test_rewards)))\n",
    "plot_rewards(test_rewards, rolling_window=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also interesting to look at the final Q-table that is learned by the agent. Note that the Q-table is of size MxNxA, where (M, N) is the size of the state space, and A is the size of the action space. We are interested in the maximum Q-value for each state, and the corresponding (best) action associated with that value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_q_table(q_table):\n",
    "    \"\"\"Visualize max Q-value for each state and corresponding action.\"\"\"\n",
    "    q_image = np.max(q_table, axis=2)  # max Q-value for each state\n",
    "    q_actions = np.argmax(q_table, axis=2)  # best action for each state\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    ax.imshow(q_image, cmap='jet');\n",
    "    for x in range(q_image.shape[0]):\n",
    "        for y in range(q_image.shape[1]):\n",
    "            ax.text(x, y, q_actions[x, y], color='white',\n",
    "                    horizontalalignment='center', verticalalignment='center')\n",
    "    ax.grid(False)\n",
    "    ax.set_title(\"Q-table, size: {}\".format(q_table.shape))\n",
    "\n",
    "\n",
    "plot_q_table(q_agent.q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise: Modify Grid\n",
    "\n",
    "Now it's your turn to play with the grid definition and see what gives you optimal results. Your agent's final performance is likely to get better if you use a finer grid, with more bins per dimension, at the cost of higher model complexity (more parameters to learn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Create a new agent with a different state space grid\n",
    "state_grid_new = create_uniform_grid(?, ?, bins=(?, ?))\n",
    "q_agent_new = QLearningAgent(env, state_grid_new)\n",
    "q_agent_new.episode_rewards = []  # initialize a list to store episode rewards for this agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train it over a desired number of episodes and analyze rewards\n",
    "# Note: This cell can be run multiple times, and episode rewards will get accumulated\n",
    "q_agent_new.episode_rewards += run(q_agent_new, env, num_episodes=1000, render_mode=None)  # accumulate episode rewards\n",
    "print(\"Completed {} total episodes with avg. reward = {}\".format(\n",
    "    len(q_agent_new.episode_rewards), np.mean(q_agent_new.episode_rewards)))\n",
    "plot_rewards(q_agent_new.episode_rewards, rolling_window=window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run in test mode and analyze rewards obtained\n",
    "test_rewards = run(q_agent_new, env, num_episodes=100, mode='test', render_mode=render_mode, render_every=10)\n",
    "print(\"[TEST] Completed {} episodes with avg. reward = {}\".format(len(test_rewards), np.mean(test_rewards)))\n",
    "plot_rewards(test_rewards, rolling_window=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize the learned Q-table\n",
    "plot_q_table(q_agent_new.q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: What conclusions can you draw from this exercise?\n",
    "- Is discretization an effective way to deal with continuous state spaces?\n",
    "- How is the learning and final performance of the agent affected by whether you choose a finer or coarser grid?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A**:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonuniform Grid\n",
    "\n",
    "Now let's try to apply the same discretization technique to a different environment, say one with a continuous state space that has more than 2 dimensions. We'll still limit ourselves to a discrete action space for simplicity.\n",
    "\n",
    "You can refer to this [table of environments](https://github.com/openai/gym/wiki/Table-of-environments) on the OpenAI Gym Wiki to choose a suitable one, e.g. `CartPole-v0` which has a 4-dimensional state space (see [wiki page](https://github.com/openai/gym/wiki/CartPole-v0) for details on what each dimension means)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Choose an environment with a higher dimensional continuous state space, but discrete action space\n",
    "env2 = gym.make('CartPole-v0')\n",
    "\n",
    "# Also make sure that the state space has definite lower and upper bounds - necessary to create a uniform grid\n",
    "print(\"State space:\", env2.observation_space)\n",
    "print(\"- low:\", env2.observation_space.low)\n",
    "print(\"- high:\", env2.observation_space.high)\n",
    "print(\"Action space:\", env2.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create an agent and initialize episode rewards list\n",
    "state_grid2 = create_uniform_grid(env2.observation_space.low, env2.observation_space.high, bins=(10, 10))\n",
    "q_agent2 = QLearningAgent(env2, state_grid2)\n",
    "q_agent2.episode_rewards = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you notice carefully, you'll see that the `low` and `high` limits for some dimensions are very large in magnitude (at least this is the case for `CartPole-v0`). This results in a discretization grid that may not work very well for the task.\n",
    "\n",
    "To get a better sense of the values we might expect to get, let's sample the state space, and plot the values obtained for each dimension separately as a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check distribution of state space values for each dimension\n",
    "state_samples = np.array([env2.observation_space.sample() for i in range(1000)])\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "for dim in range(state_samples.shape[1]):\n",
    "    axes[dim // 2, dim % 2].hist(state_samples[:, dim], bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These look like uniform distributions across the specified [`low`, `high`] ranges, don't they? But are they really representative of what values we'll see when running the environment? Perhaps not. To be sure, let's simulate running a random agent interacting with the environment and collect all the state values observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_state_samples(env, num_steps=1000):\n",
    "    \"\"\"Run environment with random actions to get realistic set of state samples.\"\"\"\n",
    "    env.reset()\n",
    "    state_samples = []\n",
    "    for i in range(num_steps):\n",
    "        state, reward, done, info = env.step(env.action_space.sample())  # random action\n",
    "        state_samples.append(state)\n",
    "        if done:\n",
    "            env.reset()\n",
    "    return np.array(state_samples)\n",
    "\n",
    "\n",
    "state_samples = get_state_samples(env2)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "for dim in range(state_samples.shape[1]):\n",
    "    ax = axes[dim // 2, dim % 2]\n",
    "    ax.hist(state_samples[:, dim], bins=20)\n",
    "    ax.set_title(\"dim #{}\".format(dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can clearly see that although the declared range for each dimension is fairly wide, the values observed in practice follow a distribution close to normal, often centered on zero or some other commonly-occuring value. In fact, for some environments, the declared range can be `(-inf, +inf)`, and yet the actual values produced might be very limited.\n",
    "\n",
    "If you have prior knowledge about the environment, you can manually specify the split points for the discretization grid to use. However, a more robust approach is to use samples obtained by simulating the environment to come up with a nonuniform grid that optimizes where you place the split points.\n",
    "\n",
    "### Quantiles\n",
    "\n",
    "_Quantiles_ of a sample are partitions that contain an equal number of values. You may be familiar with _percentile_, which is a type of quantile. Each percentile contains 1% of the total number of values. In other words, percentiles split the entire range of a given sample of values into 100 buckets such that each bucket contains the same number of values.\n",
    "\n",
    "Similarly, _quartiles_ split a sample into 4 equal-size buckets. And this idea is generalized to quantiles, where you get to specify the number of buckets or bins you want. Let's look at an example. We'll draw samples from a standard normal distribution (mean = 0, variance = 1), plot their histogram, and then change the X-axis ticks to 10-quantiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Draw samples from a Normal distribution and plot their histogram\n",
    "samples = stats.norm.rvs(loc=0.0, scale=1.0, size=1000)\n",
    "plt.figure(figsize=(16, 9))\n",
    "plt.hist(samples, bins=20);\n",
    "\n",
    "# Compute quantiles and set X-axis ticks\n",
    "quantiles = pd.Series(samples).quantile(np.linspace(0.0, 1.0, 10))\n",
    "plt.xticks(quantiles);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this allows us to have more resolution where required, and helps us avoid having to represent the entire declared range of the state space, which can be `[-inf, +inf]`.\n",
    "\n",
    "Use this idea to implement the following function. Note that you should still follow the same convention as before, i.e., given no. of bins = 10 for some dimension, you should return 9 split points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_nonuniform_grid(samples, bins=(10, 10)):\n",
    "    \"\"\"Define a nonuniform grid for discretizing a space characterized by given samples.\"\"\"\n",
    "    # TODO: Implement this\n",
    "    pass\n",
    "\n",
    "\n",
    "state_grid2 = create_nonuniform_grid(state_samples, bins=(10, 10, 10, 10))\n",
    "print(\"Nonuniform grid: [<low>, <high>] / <bins> => <splits>\")\n",
    "for l, h, splits in zip(env2.observation_space.low, env2.observation_space.high, state_grid2):\n",
    "    print(\"    [{:.3g}, {:.3g}] / {} => {}\".format(l, h, len(splits) + 1, splits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the state samples with this new grid. Labels might be hard to see, but make sure the grid spacing covers the state space as desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize state samples with non-uniform grid\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "for dim in range(state_samples.shape[1]):\n",
    "    ax = axes[dim // 2, dim % 2]\n",
    "    ax.hist(state_samples[:, dim], bins=20)\n",
    "    ax.set_title(\"dim #{}\".format(dim))\n",
    "    ax.set_xticks(state_grid2[dim])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now redefine your agent using the new grid, train it, and see if the grid enables the agent to learn the task properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Redefine agent using new non-uniform grid\n",
    "#state_samples = get_state_samples(env2, num_steps=1000)  # get state samples\n",
    "#state_grid2 = create_nonuniform_grid(state_samples, bins=(20, 20, 20, 20))  # define a non-uniform grid\n",
    "q_agent2 = QLearningAgent(env2, state_grid2)\n",
    "q_agent2.episode_rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train agent and plot rewards (accumulate episode rewards on each run)\n",
    "q_agent2.episode_rewards += run(q_agent2, env2, num_episodes=10000, render_mode=None)\n",
    "print(\"Completed {} total episodes with avg. reward = {}\".format(\n",
    "    len(q_agent2.episode_rewards), np.mean(q_agent2.episode_rewards)))\n",
    "plot_rewards(q_agent2.episode_rewards, rolling_window=window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did it do? Try training for some more iterations. And if needed, experiment with different grid sizes and/or samples to use when building the grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run in test mode and analyze rewards obtained\n",
    "test_rewards = run(q_agent2, env2, num_episodes=100, mode='test', render_mode=render_mode, render_every=10)\n",
    "print(\"[TEST] Completed {} episodes with avg. reward = {}\".format(len(test_rewards), np.mean(test_rewards)))\n",
    "plot_rewards(test_rewards, rolling_window=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try looking the Q-table of your trained agent. Note that you will have to choose 2 state dimensions to visualize, which means you will need to aggregate over the remaining dimensions. One way of aggregating would be to compute the average Q-value across all those dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize Q-table\n",
    "agg_dims = (1, 3)  # dimensions of Q-table to aggregate over\n",
    "q_table_agg = np.average(q_agent2.q_table, axis=agg_dims)  # aggregated q_table\n",
    "plot_q_table(q_table_agg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: What did you learn from this exercise?\n",
    "- Is this method of sampling a viable way to deal with state spaces that have unknown bounds or distributions?\n",
    "- Did you need to use a coarser or finer grid?\n",
    "- Was this new problem easier or harder to learn?\n",
    "- Would you be able to manually design a better grid based on your knowledge of the environment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A**:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "To learn about more advanced discretization approaches, refer to the following:\n",
    "\n",
    "- Uther, W., and Veloso, M., 1998. [Tree Based Discretization for Continuous State Space Reinforcement Learning](http://www.cs.cmu.edu/~mmv/papers/will-aaai98.pdf). In _Proceedings of AAAI, 1998_, pp. 769-774.\n",
    "- Munos, R. and Moore, A., 2002. [Variable Resolution Discretization in Optimal Control](https://link.springer.com/content/pdf/10.1023%2FA%3A1017992615625.pdf). In _Machine Learning_, 49(2), pp. 291-323."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
